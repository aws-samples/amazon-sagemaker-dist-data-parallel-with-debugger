{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moving-deficit",
   "metadata": {},
   "source": [
    "# Distributed training of a TensorFlow 2.x model with custom training loop on the Amazon SageMaker optimized TensorFlow container using the Amazon SageMaker Data Parallel library and debug using Amazon SageMaker Debugger\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, you have the option of using the built-in algorithms as well as bringing your own algorithms and frameworks.  One such framework is TensorFlow 2.x.  When performing distributed training with this framework, you can use SageMaker's Data Parallel or Model Parallel libraries.  Amazon SageMaker Debugger debugs, monitors and profiles training jobs in real time thereby helping with detecting non-converging conditions, optimizing resource utilization by eliminating bottlenecks, improving training time and reducing costs of your machine learning models.\n",
    "\n",
    "This notebook demonstrates how to use a SageMaker optimized TensorFlow 2.x container to perform distributed training on the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) using the SageMaker Data Parallel library and debug using SageMaker Debugger.  It also implements a custom training loop i.e. customizes what goes on in the fit() loop.  Finally the debugger's output is analyzed.  This will take your training script and use SageMaker in script mode.\n",
    "\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Check and upgrade required software versions](#Check%20and%20upgrade%20required%20software%20versions)\n",
    "    \n",
    "    3. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "\n",
    "    4. [Organize imports](#Organize%20imports)\n",
    "\n",
    "2. [Prepare the dataset](#Prepare%20the%20dataset)\n",
    "\n",
    "    1. [Load the dataset](#Load%20the%20dataset)\n",
    "    \n",
    "    2. [View the details of the dataset](#View%20the%20details%20of%20the%20dataset)\n",
    "    \n",
    "    3. [Visualize the dataset](#Visualize%20the%20dataset)\n",
    "    \n",
    "    4. [Normalize the dataset](#Normalize%20the%20dataset)\n",
    "    \n",
    "    5. [Save the prepared dataset locally](#Save%20the%20prepared%20dataset%20locally)\n",
    "    \n",
    "3. [Upload the prepared dataset to S3](#Upload%20the%20prepared%20dataset%20to%20S3)\n",
    "\n",
    "4. [View the training script](#View%20the%20training%20script)\n",
    "\n",
    "5. [Set the training parameters](#Set%20the%20training%20parameters)\n",
    "\n",
    "6. [Set the debugger parameters](#Set%20the%20debugger%20parameters)\n",
    "\n",
    "7. [Perform training and validation](#Perform%20training%20and%20validation)\n",
    "\n",
    "8. [View the auto-generated debugger profiling report](#View%20the%20auto-generated%20debugger%20profiling%20report)\n",
    "\n",
    "9. [Perform custom analysis of the debugger output](#Perform%20custom%20analysis%20of%20the%20debugger%20output)\n",
    "\n",
    "    1. [Get the training job](#Get%20the%20training%20job)\n",
    "\n",
    "    2. [Read the metrics](#Read%20the%20metrics)\n",
    "\n",
    "    3. [Plot the metrics](#Plot%20the%20metrics)\n",
    "    \n",
    "        1. [System metrics histogram](#System%20metrics%20histogram)\n",
    "\n",
    "        2. [Framework metrics stepline chart](#Framework%20metrics%20stepline%20chart)\n",
    "        \n",
    "        3. [Framework metrics step histogram](#Framework%20metrics%20step%20histogram)\n",
    "\n",
    "        4. [System and framework metrics timeline charts](#System%20and%20framework%20metrics%20timeline%20charts)\n",
    "\n",
    "        5. [System and framework metrics heatmap](#System%20and%20framework%20metrics%20heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-healthcare",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-knowing",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through a VPC.  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-exemption",
   "metadata": {},
   "source": [
    "###  B. Check and upgrade required software versions\n",
    "This notebook requires:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [TensorFlow version 2.x with SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html)\n",
    "* [Python 3.6.x](https://www.python.org/downloads/release/python-360/)\n",
    "* [SMDebug](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-analyze-data.html)\n",
    "\n",
    "NOTE: If you get 'module not found' errors in the following cell, then uncomment the appropriate installation commands and install the modules. Also, uncomment and run the kernel shutdown command. When the kernel comes back, comment out the installation and kernel shutdown commands and run the following cell. Now, you should not see any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-disease",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import sagemaker\n",
    "import smdebug\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# Install/upgrade sagemaker, smdebug and tensorflow\n",
    "#!{sys.executable} -m pip install -U sagemaker smdebug\n",
    "#!{sys.executable} -m pip install -U tensorflow==2.3.1\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "# Get the current installed version of sagemaker, tensorflow and python\n",
    "print('SageMaker Python SDK version : {}'.format(sagemaker.__version__))\n",
    "print('TensorFlow version : {}'.format(tf.__version__))\n",
    "print('Python version : {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-gasoline",
   "metadata": {},
   "source": [
    "###  C. Check and configure security permissions\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell.\n",
    "\n",
    "NOTE: This role should have the following permissions,\n",
    "\n",
    "1. Full access to the S3 bucket that will be used to store training and output data.\n",
    "2. Full access to launch training instances.\n",
    "3. Access to write to CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-pregnancy",
   "metadata": {},
   "source": [
    "###  D. Organize imports\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-harbor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sagemaker.debugger import (ProfilerConfig,\n",
    "                                FrameworkProfile,\n",
    "                                CollectionConfig,\n",
    "                                DebuggerHookConfig,\n",
    "                                DetailedProfilingConfig, \n",
    "                                DataloaderProfilingConfig, \n",
    "                                PythonProfilingConfig,\n",
    "                                Rule,\n",
    "                                PythonProfiler,\n",
    "                                cProfileTimer,\n",
    "                                ProfilerRule,\n",
    "                                rule_configs)\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from smdebug.profiler.analysis.notebook_utils.metrics_histogram import MetricsHistogram\n",
    "from smdebug.profiler.analysis.notebook_utils.step_timeline_chart import StepTimelineChart\n",
    "from smdebug.profiler.analysis.notebook_utils.step_histogram import StepHistogram\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "from smdebug.profiler.analysis.notebook_utils.heatmap import Heatmap\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-hotel",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "The [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) consists of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images.  These categories are mapped to integers from 0 to 9 and represent the following class labels,\n",
    "\n",
    "* 0: T-shirt/top\n",
    "* 1: Trouser\n",
    "* 2: Pullover\n",
    "* 3: Dress\n",
    "* 4: Coat\n",
    "* 5: Sandal\n",
    "* 6: Shirt\n",
    "* 7: Sneaker\n",
    "* 8: Bag\n",
    "* 9: Ankle boot\n",
    "\n",
    "The following steps will help with preparing the dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-bachelor",
   "metadata": {},
   "source": [
    "### A) Load the dataset\n",
    "\n",
    "Load the pre-shuffled train and test data with the keras.datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-yugoslavia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-momentum",
   "metadata": {},
   "source": [
    "### B) View the details of the dataset\n",
    "Print the shape of the data and you will notice that they are 28x28 pixels. There are 60,000 images in the training data and 10,000 images in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-acoustic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize the dataset\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-order",
   "metadata": {},
   "source": [
    "### C) Visualize the dataset\n",
    "Randomly display the images and labels of `sample_size` number of images from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-picture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Randomly display the images and labels of n (sample_size) images from the test dataset\n",
    "\n",
    "sample_size = 50\n",
    "\n",
    "random_indexes = np.random.randint(0, len(x_test), sample_size)\n",
    "sample_images = x_test[random_indexes]\n",
    "sample_labels = y_test[random_indexes]\n",
    "sample_predictions = None\n",
    "num_rows = 5\n",
    "num_cols = 10\n",
    "plot_title = None\n",
    "fig_size = None\n",
    "assert sample_images.shape[0] == num_rows * num_cols\n",
    "\n",
    "# Labels\n",
    "FASHION_LABELS = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    sns.set_context(\"notebook\", font_scale=1.1)\n",
    "    sns.set_style({\"font.sans-serif\": [\"Verdana\", \"Arial\", \"Calibri\", \"DejaVu Sans\"]})\n",
    "    f, ax = plt.subplots(num_rows, num_cols, figsize=((14, 9) if fig_size is None else fig_size),\n",
    "        gridspec_kw={\"wspace\": 0.02, \"hspace\": 0.30}, squeeze=True)\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_cols):\n",
    "            image_index = r * num_cols + c\n",
    "            ax[r, c].axis(\"off\")\n",
    "            ax[r, c].imshow(sample_images[image_index], cmap=\"Greys\")\n",
    "            if sample_predictions is None:\n",
    "                title = ax[r, c].set_title(\"%s\" % FASHION_LABELS[sample_labels[image_index]])\n",
    "            else:\n",
    "                true_label = sample_labels[image_index]\n",
    "                pred_label = sample_predictions[image_index]\n",
    "                prediction_matches_true = (sample_labels[image_index] == sample_predictions[image_index])\n",
    "                if prediction_matches_true:\n",
    "                    title = FASHION_LABELS[true_label]\n",
    "                    title_color = 'g'\n",
    "                else:\n",
    "                    title = '%s/%s' % (FASHION_LABELS[true_label], FASHION_LABELS[pred_label])\n",
    "                    title_color = 'r'\n",
    "                title = ax[r, c].set_title(title)\n",
    "                plt.setp(title, color=title_color)\n",
    "    if plot_title is not None:\n",
    "        f.suptitle(plot_title)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-witch",
   "metadata": {},
   "source": [
    "The pixel values of the images fall in the range of 0 to 255. You can verify this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-delicious",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-blend",
   "metadata": {},
   "source": [
    "### D) Normalize the dataset\n",
    "As the pixel values range from 0 to 255, it is important to normalize them to a range from 0 to 1. This can be done by dividing these values by 255. This has to be done for both training and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-melbourne",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-mineral",
   "metadata": {},
   "source": [
    "### E) Save the prepared dataset locally\n",
    "Save the normalized dataset to local directories.  Create the directories if they don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-slovenia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir_name = 'tf2_fashion_mnist_custom_smd_debugger'\n",
    "\n",
    "# Create the local directories\n",
    "data_dir = os.path.join(os.getcwd(), 'data/{}'.format(dir_name))\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "train_dir = os.path.join(os.getcwd(), 'data/{}/train'.format(dir_name))\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "test_dir = os.path.join(os.getcwd(), 'data/{}/test'.format(dir_name))\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Save the prepared dataset (in numpy format) to the local directories\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-mixer",
   "metadata": {},
   "source": [
    "## 3. Upload the prepared dataset to S3\n",
    "Upload the dataset from the local directories to appropriate sub-directories in the required S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-finnish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the S3 bucket prefix\n",
    "s3_bucket = '<Specify the S3 bucket name>'\n",
    "\n",
    "# Generate the sub-folder names\n",
    "train_data_s3_prefix = '{}/data/train'.format(dir_name)\n",
    "test_data_s3_prefix = '{}/data/test'.format(dir_name)\n",
    "\n",
    "# Upload the data to S3\n",
    "train_data_s3_full_path = sagemaker.Session().upload_data(path='./data/{}/train/'.format(dir_name), bucket=s3_bucket, key_prefix=train_data_s3_prefix)\n",
    "test_data_s3_full_path = sagemaker.Session().upload_data(path='./data/{}/test/'.format(dir_name), bucket=s3_bucket, key_prefix=test_data_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-royalty",
   "metadata": {},
   "source": [
    "## 4. View the training script\n",
    "View the script that will be used for training the model.  This should exist in a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-kernel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat scripts/train_tf2_fashion_mnist_custom_smd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-dealing",
   "metadata": {},
   "source": [
    "## 5. Set the training parameters\n",
    "\n",
    "1. Inputs - S3 locations for training and test data.\n",
    "2. Hyperparameters.\n",
    "3. SageMaker Data Parallel distribution parameters.\n",
    "4. Training instance details:\n",
    "\n",
    "    1. Instance count (NOTE: Requires minimum of 2)\n",
    "    \n",
    "    2. Instance type (NOTE: SageMaker Data Parallel supported instance types are mentioned [here](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-faq.html))\n",
    "    \n",
    "    3. The max run time of the training job\n",
    "    \n",
    "    4. (Optional) Use Spot instances\n",
    "    \n",
    "    5. (Optional) The max wait for Spot instances, if using Spot.  This should be larger than the max run time.\n",
    "    \n",
    "5. Base job name, Tensorflow framework version and Python version.\n",
    "6. Names of the training script and the local directory where it is located.\n",
    "7. Logging level of the SageMaker optimized Tensorflow 2.x container.\n",
    "8. Appropriate local and S3 directories that will be used by the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-indianapolis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the input data paths\n",
    "inputs = {'train':train_data_s3_full_path, 'test':test_data_s3_full_path}\n",
    "\n",
    "# Set the hyperparameters\n",
    "hyperparameters = {'epochs': 25,\n",
    "                   'batch_size': 32,\n",
    "                   'learning_rate': 0.001,\n",
    "                   'decay': 1e-6}\n",
    "\n",
    "# Set the distribution\n",
    "distribution = { \"smdistributed\":\n",
    "                { \"dataparallel\":\n",
    "                 { \"enabled\": True\n",
    "                 }\n",
    "                }\n",
    "               } \n",
    "\n",
    "# Set the instance count, instance type, options to use Spot instances, job name and other parameters\n",
    "train_instance_count = 2\n",
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "#use_spot_instances = True\n",
    "#spot_max_wait_time_in_seconds = 9000\n",
    "use_spot_instances = False\n",
    "spot_max_wait_time_in_seconds = None\n",
    "max_run_time_in_seconds = 7200\n",
    "\n",
    "base_job_name = 'train-tf2-fashion-mnist-custom-smd-debugger'\n",
    "framework_version = '2.4.1'\n",
    "py_version = 'py37'\n",
    "\n",
    "# Set the training script related parameters\n",
    "train_script_dir = 'scripts'\n",
    "#train_script = 'train_tf2_fashion_mnist.py'\n",
    "train_script = 'train_tf2_fashion_mnist_custom_smd.py'\n",
    "\n",
    "# Set the training container related parameters\n",
    "container_log_level = logging.INFO\n",
    "\n",
    "# Location where the model checkpoints will be stored locally in the container before being uploaded to S3\n",
    "model_checkpoint_local_dir = '/opt/ml/checkpoints/'\n",
    "\n",
    "# Location where the trained model will be stored locally in the container before being uploaded to S3\n",
    "model_local_dir = '/opt/ml/model'\n",
    "\n",
    "# Location in S3 where the training scripts will be copied\n",
    "code_location = 's3://{}/{}/scripts'.format(s3_bucket, dir_name)\n",
    "\n",
    "# Location in S3 where the model checkpoint will be stored\n",
    "model_checkpoint_s3_path = 's3://{}/{}/checkpoint/'.format(s3_bucket, dir_name)\n",
    "\n",
    "# Location in S3 where the trained model and debugger output will be stored\n",
    "model_and_debugger_output_s3_path = 's3://{}/{}/output/'.format(s3_bucket, dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-indonesian",
   "metadata": {},
   "source": [
    "## 6. Set the debugger parameters\n",
    "\n",
    "1. **Profile config** - configure how to collect system metrics and framework metrics from your training job and save into your secured S3 bucket URI or local machine.\n",
    "\n",
    "    1. [Monitoring hardware system resource utilization](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-configure-system-monitoring.html)\n",
    "  \n",
    "    2. [Framework profiling](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-configure-framework-profiling.html)\n",
    "  \n",
    "2. **Debugger hook config** - configure how to collect output tensors from your training job and save into your secured S3 bucket URI or local machine.  For more info, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-configure-hook.html).\n",
    "\n",
    "3. **Rules** - configure this parameter to enable Debugger built-in rules that you want to run in parallel. The rules automatically analyze your training job and find training issues. The ProfilerReport rule saves the Debugger profiling reports in your secured S3 bucket URI.  For more info, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/use-debugger-built-in-rules.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-scholarship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Location in S3 where the debugger output will be stored is mentioned in the previous step\n",
    "\n",
    "# Set the profile config for both system and framework metrics\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis = 1000,\n",
    "    framework_profile_params = FrameworkProfile(\n",
    "        detailed_profiling_config = DetailedProfilingConfig(\n",
    "            start_step = 5, \n",
    "            num_steps = 10\n",
    "        ),\n",
    "        dataloader_profiling_config = DataloaderProfilingConfig(\n",
    "            start_step = 7, \n",
    "            num_steps = 10\n",
    "        ),\n",
    "        python_profiling_config = PythonProfilingConfig(\n",
    "            start_step = 9, \n",
    "            num_steps = 10,\n",
    "            python_profiler = PythonProfiler.CPROFILE, \n",
    "            cprofile_timer = cProfileTimer.TOTAL_TIME\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set the debugger hook config to save tensors\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "    collection_configs = [\n",
    "        CollectionConfig(name = \"weights\"),\n",
    "        CollectionConfig(name = \"gradients\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set the rules to analyze tensors emitted during training\n",
    "rules = [\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.stalled_training_rule())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-flashing",
   "metadata": {},
   "source": [
    "## 7. Perform training, validation and testing\n",
    "Prepare the estimator and call the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-breach",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the estimator\n",
    "estimator = TensorFlow(\n",
    "    source_dir=train_script_dir,\n",
    "    entry_point=train_script,\n",
    "    code_location=code_location,\n",
    "    checkpoint_local_path=model_checkpoint_local_dir,\n",
    "    checkpoint_s3_uri=model_checkpoint_s3_path,\n",
    "    model_dir=model_local_dir,\n",
    "    output_path=model_and_debugger_output_s3_path,\n",
    "    distribution=distribution,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=train_instance_count,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_wait=spot_max_wait_time_in_seconds,\n",
    "    max_run=max_run_time_in_seconds,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name=base_job_name,\n",
    "    framework_version=framework_version,\n",
    "    py_version=py_version,\n",
    "    container_log_level=container_log_level,\n",
    "    script_mode=True,\n",
    "    disable_profiler=False,\n",
    "    profiler_config=profiler_config,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    rules=rules)\n",
    "\n",
    "# Perform the training\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-quality",
   "metadata": {},
   "source": [
    "## 8. View the auto-generated debugger profiling report\n",
    "The debugger's auto-generated profiling report will be stored in the S3 directory specified in earlier steps.  You can view it here.\n",
    "\n",
    "For information on how to read the report, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-profiling-report.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-heather",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the S3 path to the debugger's auto-generated profiling report\n",
    "profiling_report_s3_prefix = '{}/output/{}/rule-output/ProfilerReport/profiler-output/profiler-report.html'.format(dir_name,\n",
    "                                                                             estimator.latest_training_job.job_name)\n",
    "profiling_report = sagemaker.Session().read_s3_file(s3_bucket, profiling_report_s3_prefix)\n",
    "\n",
    "\n",
    "# Print debugger's auto-generated profiling report location\n",
    "display(HTML(profiling_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-brush",
   "metadata": {},
   "source": [
    "## 9. Perform custom analysis of the debugger output\n",
    "The debugger's output will be stored in the S3 directories specified in earlier steps.  In this step, we will read that data and display them through various visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-decimal",
   "metadata": {},
   "source": [
    "### A. Get the training job\n",
    "Get the training job object from the estimator object used for training in the previous step.  This is required to read the metrics from the debugger's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-weapon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This assumes that the job was trained in the same AWS region as the S3 bucket where the debugger output is stored\n",
    "# If not, then make appropriate changes to the following code\n",
    "tj = TrainingJob(estimator.latest_training_job.job_name, sagemaker.Session().boto_region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-cambodia",
   "metadata": {},
   "source": [
    "### B. Read the metrics\n",
    "1. Wait for the the system and framework metrics to be available.\n",
    "2. Get the reader objects for both of these metrics.\n",
    "3. Refresh the event file lists that contains these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-spencer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wait for the data to be available\n",
    "tj.wait_for_sys_profiling_data_to_be_available()\n",
    "tj.wait_for_framework_profiling_data_to_be_available()\n",
    "# Get the metrics reader\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "framework_metrics_reader = tj.get_framework_metrics_reader()\n",
    "# Refresh the event file list\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "framework_metrics_reader.refresh_event_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-underwear",
   "metadata": {},
   "source": [
    "### C. Plot the metrics\n",
    "Plot visualizations for the metrics read in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-orchestra",
   "metadata": {},
   "source": [
    "#### a. System metrics histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-composite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_histogram = MetricsHistogram(system_metrics_reader)\n",
    "metrics_histogram.plot(\n",
    "    starttime=0, \n",
    "    endtime=system_metrics_reader.get_timestamp_of_latest_available_file(), \n",
    "    select_dimensions=[\"CPU\", \"GPU\", \"I/O\"],\n",
    "    select_events=[\"total\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-print",
   "metadata": {},
   "source": [
    "#### b. Framework metrics stepline chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-somalia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_step_timeline_chart = StepTimelineChart(framework_metrics_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-disney",
   "metadata": {},
   "source": [
    "#### c. Framework metrics step histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-pottery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_histogram = StepHistogram(framework_metrics_reader)\n",
    "step_histogram.plot(\n",
    "    starttime=step_histogram.last_timestamp - 5 * 1000 * 1000, \n",
    "    endtime=step_histogram.last_timestamp, \n",
    "    show_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-agenda",
   "metadata": {},
   "source": [
    "#### d. System and framework metrics timeline charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-denmark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader, \n",
    "    framework_metrics_reader,\n",
    "    select_dimensions=[\"CPU\", \"GPU\", \"I/O\"],\n",
    "    select_events=[\"total\"] \n",
    ")\n",
    "\n",
    "view_timeline_charts.plot_detailed_profiler_data([700,710]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-flush",
   "metadata": {},
   "source": [
    "#### e. System and framework metrics heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-harvard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_heatmap = Heatmap(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader,\n",
    "    select_dimensions=[\"CPU\", \"GPU\", \"I/O\"],\n",
    "    select_events=[\"total\"],\n",
    "    plot_height=450\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-conditioning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
